#!/bin/bash

# Spark Connector installer library for Greenplum Installer
# Supports Greenplum-Spark Connector (Scala 2.12 tar.gz placed in files/)
# - Finds connector tarball in files/
# - Distributes to all hosts
# - Unpacks into /usr/local/greenplum-spark-connector and configures env

source "$(dirname "${BASH_SOURCE[0]}")/logging.sh"
source "$(dirname "${BASH_SOURCE[0]}")/ssh.sh"
source "$(dirname "${BASH_SOURCE[0]}")/config.sh"

readonly SPARK_CONNECTOR_TARBALL_PATTERNS=(
  "greenplum-connector-apache-spark-*.tar.gz"
  "greenplum-connector-apache-spark-scala_*.tar.gz"
)

should_install_spark_connector() {
  local v="${INSTALL_SPARK_CONNECTOR}"
  local v_lc=$(printf '%s' "$v" | tr '[:upper:]' '[:lower:]')
  if [ "$v_lc" = "true" ]; then
    return 0
  fi
  if find_spark_connector_tarball "$INSTALL_FILES_DIR" >/dev/null 2>&1; then
    return 0
  fi
  return 1
}

find_spark_connector_tarball() {
  local install_files_dir="${1:-files}"
  local found=""
  for pattern in "${SPARK_CONNECTOR_TARBALL_PATTERNS[@]}"; do
    found=$(find "$install_files_dir" -maxdepth 1 -type f -name "$pattern" 2>/dev/null | head -n1)
    [ -n "$found" ] && { echo "$found"; return 0; }
  done
  return 1
}

distribute_spark_connector() {
  local tarball="$1"
  local hosts=("${@:2}")
  log_info "Distributing Spark Connector to hosts..."
  for host in "${hosts[@]}"; do
    if ! ssh_copy_file "$tarball" "$host" "/tmp/"; then
      log_error "Failed to copy Spark Connector to $host"
      return 1
    fi
  done
  log_success "Spark Connector tarball distributed"
}

install_spark_connector_single() {
  local host="$1"
  local tarball="$2"
  local sudo_password="$3"
  local remote_tarball="/tmp/$(basename "$tarball")"
  log_info_with_timestamp "Installing Spark Connector on $host..."
  local remote_script="
    set -e
    echo 'Preparing destination directory...'
    echo '$sudo_password' | sudo -S mkdir -p /usr/local/greenplum-spark-connector
    echo '$sudo_password' | sudo -S tar -xzf $remote_tarball -C /usr/local/greenplum-spark-connector --strip-components=1 || \
      (echo 'Fallback: extracting without strip'; echo '$sudo_password' | sudo -S tar -xzf $remote_tarball -C /usr/local/greenplum-spark-connector)
    echo '$sudo_password' | sudo -S chown -R gpadmin:gpadmin /usr/local/greenplum-spark-connector
    rm -f $remote_tarball
    echo 'Spark Connector installed at /usr/local/greenplum-spark-connector'
  "
  if ssh_execute "$host" "$remote_script"; then
    log_success_with_timestamp "Spark Connector installed on $host"
  else
    log_error_with_timestamp "Spark Connector installation failed on $host"
    return 1
  fi
}

install_spark_connector_binaries() {
  local args=("$@")
  local last_index=$((${#args[@]}-1))
  local tarball="${args[$last_index]}"
  unset 'args[$last_index]'
  local hosts=("${args[@]}")
  log_info "Installing Spark Connector on all hosts..."
  local failed=()
  for host in "${hosts[@]}"; do
    if ! install_spark_connector_single "$host" "$tarball" "$SUDO_PASSWORD"; then
      failed+=("$host")
    fi
  done
  if [ ${#failed[@]} -gt 0 ]; then
    log_error "Spark Connector installation failed on: ${failed[*]}"
    return 1
  fi
  log_success "Spark Connector installed on all hosts"
}

setup_spark_connector_environment() {
  local hosts=("$@")
  for host in "${hosts[@]}"; do
    log_info "Configuring gpadmin environment for Spark Connector on $host..."
    local temp_script="/tmp/setup_spark_env_$$.sh"
    cat > "$temp_script" << 'EOF'
#!/bin/bash

dest_dir=/usr/local/greenplum-spark-connector

if [ -d "$dest_dir" ]; then
  if ! grep -q "SPARK_CONNECTOR_HOME" /home/gpadmin/.bashrc 2>/dev/null; then
    cat >> /home/gpadmin/.bashrc << SCENV

# Spark Connector Environment - Auto-generated by installer
export SPARK_CONNECTOR_HOME=$dest_dir
export PATH=\$SPARK_CONNECTOR_HOME/bin:\$PATH
SCENV
  fi
  if ! grep -q "SPARK_CONNECTOR_HOME" /home/gpadmin/.bash_profile 2>/dev/null; then
    cat >> /home/gpadmin/.bash_profile << SCENV

# Spark Connector Environment - Auto-generated by installer
export SPARK_CONNECTOR_HOME=$dest_dir
export PATH=\$SPARK_CONNECTOR_HOME/bin:\$PATH
SCENV
  fi
  chown gpadmin:gpadmin /home/gpadmin/.bashrc /home/gpadmin/.bash_profile
fi
EOF
    ssh_copy_file "$temp_script" "$host" "/tmp/"
    ssh_execute "$host" "sudo bash /tmp/$(basename $temp_script) && rm -f /tmp/$(basename $temp_script)"
    rm -f "$temp_script"
  done
  log_success "Spark Connector environment configured for gpadmin"
}

install_spark_connector_full() {
  read -r -a all_hosts <<< "$(get_all_hosts)"
  local tarball
  if ! tarball=$(find_spark_connector_tarball "$INSTALL_FILES_DIR"); then
    log_error "Spark Connector tarball not found in '$INSTALL_FILES_DIR'"
    return 1
  fi
  distribute_spark_connector "$tarball" "${all_hosts[@]}"
  install_spark_connector_binaries "${all_hosts[@]}" "$tarball"
  setup_spark_connector_environment "${all_hosts[@]}"
}

# Verify Spark Connector by asserting JAR exists and showing a spark-shell command suggestion
verify_spark_connector_installation() {
  local coordinator_host="$1"
  log_info "Verifying Spark Connector installation..."
  local verify_script="
    set -e
    if [ -d /usr/local/greenplum-spark-connector ]; then
      echo 'Connector directory exists.'
      ls -1 /usr/local/greenplum-spark-connector | grep -E 'greenplum-connector-apache-spark-.*\\.jar' -m1 || true
    else
      echo 'Connector directory not found.'
      exit 1
    fi
  "
  if ssh_execute "$coordinator_host" "$verify_script"; then
    log_success "Spark Connector appears installed. To use:"
    log_info "spark-shell --jars $SPARK_CONNECTOR_HOME/greenplum-connector-apache-spark-scala_2.12-<ver>.jar"
  else
    log_warn "Spark Connector verification failed."
  fi
}


